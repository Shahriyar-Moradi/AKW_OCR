{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize the EasyOCR Reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "IMAGE_PATH = 'images/Kyrgyzstan.jpg'\n",
    "\n",
    "img = cv2.imread(IMAGE_PATH)\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not found or unable to read the image.\")\n",
    "# Function to enhance low-contrast images\n",
    "def enhance_contrast(image):\n",
    "    # Convert image to LAB color space\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "\n",
    "    # Convert back to BGR color space\n",
    "    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced_img\n",
    "\n",
    "# Function to save JSON output for each image\n",
    "def save_json_output(image_path, detections):\n",
    "    json_output = {\n",
    "        'image_path': image_path,\n",
    "        'recognized_text': [\n",
    "            {\n",
    "                'Detected text': detection['text'],\n",
    "                'Bounding box': detection['bounding_box']\n",
    "            } for detection in detections\n",
    "        ]\n",
    "    }\n",
    "    json_filename = image_path.rsplit('.', 1)[0] + '.json'\n",
    "\n",
    "    json_output_converted = {\n",
    "        'image_path': json_output['image_path'],\n",
    "        'recognized_texts': [\n",
    "            {\n",
    "                'Detected text': d['Detected text'],\n",
    "                'Bounding box': [list(map(int, point)) for point in d['Bounding box']]\n",
    "            } for d in json_output['recognized_text']\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(json_output_converted, json_file, indent=4)\n",
    "\n",
    "# Function to manually verify the accuracy of the recognized text\n",
    "def manual_verification(image_path, detections):\n",
    "    print(f\"Manual verification for image: {image_path}\")\n",
    "    for i, detection in enumerate(detections):\n",
    "        bounding_box = detection['bounding_box']\n",
    "        text = detection['text']\n",
    "        confidence = detection['confidence']\n",
    "        \n",
    "        print(f\"\\nDetection {i + 1}:\")\n",
    "        print(f\"Detected text: {text}\")\n",
    "        print(f\"Bounding box: {bounding_box}\")\n",
    "        print(f\"Confidence: {confidence}\")\n",
    "        \n",
    "        # actual_text = input(\"Enter the actual text (or press Enter to accept the detected text): \").strip()\n",
    "        \n",
    "        # if actual_text:\n",
    "        #     detection['text'] = actual_text\n",
    "        \n",
    "        print(f\"Final text for detection {i + 1}: {detection['text']}\")\n",
    "\n",
    "# Function to save the processed image with bounding boxes\n",
    "def save_processed_image(image_path, img):\n",
    "    processed_image_path = image_path.rsplit('.', 1)[0] + '_processed.jpg'\n",
    "    cv2.imwrite(processed_image_path, img)\n",
    "    print(f\"Processed image saved at: {processed_image_path}\")\n",
    "\n",
    "# Read the image\n",
    "\n",
    "\n",
    "# Enhance the image for better OCR accuracy\n",
    "img = enhance_contrast(img)\n",
    "\n",
    "# Perform OCR\n",
    "result = reader.readtext(img)\n",
    "\n",
    "# List to collect OCR results\n",
    "detections = []\n",
    "\n",
    "# Iterate through detections\n",
    "for detection in result:\n",
    "    top_left = tuple(map(int, detection[0][0]))\n",
    "    bottom_right = tuple(map(int, detection[0][2]))\n",
    "    text = detection[1]\n",
    "    confidence = detection[2]\n",
    "    \n",
    "    # Draw rectangle and put text\n",
    "    img = cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), 3)\n",
    "    \n",
    "    # Collect the results\n",
    "    detections.append({\n",
    "        'bounding_box': detection[0],\n",
    "        'text': text,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "# Save the processed image with bounding boxes\n",
    "save_processed_image(IMAGE_PATH, img)\n",
    "\n",
    "# Convert BGR image to RGB for displaying with matplotlib\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "# Function to calculate accuracy (simple example)\n",
    "def calculate_accuracy(ground_truth, recognized_text):\n",
    "    gt_words = ground_truth.split()\n",
    "    rec_words = recognized_text.split()\n",
    "    correct = sum(1 for gt, rec in zip(gt_words, rec_words) if gt == rec)\n",
    "    accuracy = (correct / len(gt_words)) * 100 if gt_words else 0\n",
    "    return accuracy\n",
    "\n",
    "# Save the JSON output\n",
    "save_json_output(IMAGE_PATH, detections)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
